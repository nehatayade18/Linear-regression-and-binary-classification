{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fzMyE386oaaV"
   },
   "source": [
    "\n",
    "**Fall 2019**\n",
    "\n",
    "**P556: Applied Machine Learning**\n",
    "\n",
    "**Assignment #1**\n",
    "\n",
    "**Due date: September 18, 2019. 11:59 PM**\n",
    "\n",
    "DO NOT CHANGE THE FUNCTION DEFINITIONS UNLESS APPROVED BY AN AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJz8ZxU7opMy"
   },
   "source": [
    "# Problem #1: Linear Regression\n",
    "\n",
    "##  Problem 1.1 (25 points)\n",
    "\n",
    "Implement linear regression using gradient descent. Your implementation should be able to handle simple and multiple linear regression.\n",
    "\n",
    "Note 1: by implementation we mean that everything has to be written from scratch and that you cannot call a linear regression function from a library, such as sklearn. Usage of standard libraries, such as numpy, pandas, etc., is fine. If you are unsure about whether a library can be used, please contact the AIs well in advance of the submission date.\n",
    "\n",
    "Note 2: You are free to use sklearn to test whether your results match that from a battle-tested library. This is a great way to know before hand whether your submission is correct. Make sure to use the same parameters on both models before you spend an eternity debugging code that is correct but not returning the same values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dW1xPyXPoonO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated cost before updation target    1184.293834\n",
      "dtype: float64\n",
      "Cost after updation is: target    328.946928\n",
      "dtype: float64\n",
      "Regression coefficients: [[ 2.25328063e+01]\n",
      " [-9.21625901e-01]\n",
      " [ 1.07012433e+00]\n",
      " [ 1.05613181e-01]\n",
      " [ 6.86780128e-01]\n",
      " [-2.05010502e+00]\n",
      " [ 2.68072587e+00]\n",
      " [ 1.39686204e-02]\n",
      " [-3.10628267e+00]\n",
      " [ 2.57318542e+00]\n",
      " [-1.97576133e+00]\n",
      " [-2.05721328e+00]\n",
      " [ 8.48667146e-01]\n",
      " [-3.74018384e+00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.linear_regression at 0x1d4d7d0f748>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "ds=load_boston()\n",
    "\n",
    "dfx = pd.DataFrame(ds.data, columns=ds.feature_names)\n",
    "dfy = pd.DataFrame(ds.target,columns=[\"target\"])\n",
    "\n",
    "# defining feature matrix(X) and response vector(Y)\n",
    "X=dfx\n",
    "Y=dfy\n",
    "X = (X - np.mean(X))\n",
    "X=X/np.std(X)\n",
    "#print(X.shape)\n",
    "\n",
    "\n",
    "class linear_regression:\n",
    "    def __init__(self, learning_rate=0.05, iterations=1000, fit_intercept=True, normalize=False, coef=None):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.normalize = normalize\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.coef = coef\n",
    "        \n",
    "    def predict(self, X):\n",
    "\n",
    "        Y_predict = np.dot(X,self.fit_intercept) \n",
    "        return Y_predict\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        m=len(Y)\n",
    "        self.fit_intercept = np.zeros([X.shape[1]+1,1])  #size 14*1\n",
    "        ones = np.ones((m,1))  #size 506\n",
    "        X = np.hstack((ones, X))\n",
    "        self.cost=[]\n",
    "        Y_predict = self.predict(X) - Y\n",
    "        cost1=np.sum(np.power((Y_predict), 2)) / (2*m)\n",
    "        print('The estimated cost before updation',cost1)\n",
    "        \n",
    "        for i in range(self.iterations):\n",
    "            Y_predict = self.predict(X) - Y\n",
    "            error = np.dot(X.T, Y_predict)\n",
    "            self.fit_intercept = self.fit_intercept - (self.learning_rate/m) * error\n",
    "            costcal=np.sum(np.power((Y_predict), 2)) / (2*m)\n",
    "            self.cost.append(costcal)\n",
    "        print('Cost after updation is:',costcal)\n",
    "        print('Regression coefficients:',self.fit_intercept)\n",
    "        return self\n",
    "        \n",
    "lr=linear_regression()\n",
    "lr.fit(X,Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Man3c1JrhVbr"
   },
   "source": [
    "## Problem 1.2 (10 points)\n",
    "\n",
    "- Split the Boston Housing dataset into train and test sets (70% and 30%, respectively) (5 points). \n",
    "- Fit your linear regression implementation using the training set and print your model's coefficients. Make predictions for the test set using your fitted model (5 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VEFBL6WwhXUz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection  import train_test_split\n",
    "ds=load_boston()\n",
    "\n",
    "dfx = pd.DataFrame(ds.data, columns=ds.feature_names)\n",
    "dfy = pd.DataFrame(ds.target,columns=[\"target\"])\n",
    "\n",
    "# defining feature matrix(X) and response vector(Y)\n",
    "X=dfx\n",
    "Y=dfy\n",
    "X = (X - np.mean(X))\n",
    "X=X/np.std(X)\n",
    "\n",
    "class linear_regression:\n",
    "    def __init__(self, learning_rate=0.05, iterations=1000, fit_intercept=True, normalize=False, coef=None):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.normalize = normalize\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.coef = coef\n",
    "        \n",
    "    def predict(self, X):\n",
    "\n",
    "        Y_predict = np.dot(X,self.fit_intercept) \n",
    "        return Y_predict\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        X = (X - np.mean(X))\n",
    "        X=X/np.std(X)\n",
    "        m=len(Y)\n",
    "        self.fit_intercept = np.zeros([X.shape[1]+1,1])  #size 14*1\n",
    "        ones = np.ones((m,1))  #size 506\n",
    "        X = np.hstack((ones, X))#size 506*1\n",
    "        self.cost=[]\n",
    "        Y_predict = self.predict(X) - Y\n",
    "        cost1=np.sum(np.power((Y_predict), 2)) / (2*m)\n",
    "        print('The estimated cost before updation',cost1)\n",
    "        \n",
    "        for i in range(self.iterations):\n",
    "            Y_predict = self.predict(X) - Y\n",
    "            error = np.dot(X.T, Y_predict)\n",
    "            self.fit_intercept = self.fit_intercept - (self.learning_rate/m) * error\n",
    "            costcal=np.sum(np.power((Y_predict), 2)) / (2*m)\n",
    "            self.cost.append(costcal)\n",
    "        print('Cost after updation is:',costcal)\n",
    "        print('Regression coefficients:',self.fit_intercept)\n",
    "        return self\n",
    "\n",
    "trainset_X, testset_X, trainset_Y, testset_Y=train_test_split(dfx,dfy,test_size=0.30)\n",
    "print(\"Input traning set size: \"+str(trainset_X.shape))\n",
    "print(\"Input test set size: \"+str(testset_X.shape))\n",
    "print(\"Output traning set size: \"+str(trainset_Y.shape))\n",
    "print(\"Output test set size: \"+str(testset_Y.shape))\n",
    "print(\"\\nDataset split successful\")\n",
    "\n",
    "\n",
    "#print(trainset_X)#354*13\n",
    "#print(trainset_Y)#354*1\n",
    "\n",
    "lr=linear_regression()\n",
    "lr.fit(trainset_X, trainset_Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FXqtD_KhZwV"
   },
   "source": [
    "## Problem 1.3 (10 points)\n",
    "\n",
    "Identify the variable or set of variables that will minimize the mean square error (MSE). Hint: this is where your function being able to handle simple and multiple regression becomes useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sFlcvY_piKus"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection  import train_test_split\n",
    "ds=load_boston()\n",
    "\n",
    "dfx = pd.DataFrame(ds.data, columns=ds.feature_names)\n",
    "dfy = pd.DataFrame(ds.target,columns=[\"target\"])\n",
    "\n",
    "# defining feature matrix(X) and response vector(Y)\n",
    "X=dfx\n",
    "Y=dfy\n",
    "X = (X - np.mean(X))\n",
    "X=X/np.std(X)\n",
    "\n",
    "class linear_regression:\n",
    "    def __init__(self, learning_rate=0.05, iterations=1000, fit_intercept=True, normalize=False, coef=None):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.normalize = normalize\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.coef = coef\n",
    "        \n",
    "    def predict(self, X):\n",
    "\n",
    "        Y_predict = np.dot(X,self.fit_intercept) \n",
    "        return Y_predict\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        X = (X - np.mean(X))\n",
    "        X=X/np.std(X)\n",
    "        m=len(Y)\n",
    "        self.fit_intercept = np.zeros([X.shape[1]+1,1])  #size 14*1\n",
    "        ones = np.ones((m,1))  #size 506\n",
    "        X = np.hstack((ones, X))#size 506*1\n",
    "        self.mse=[]\n",
    "        Y_predict = self.predict(X) - Y\n",
    "        cost1=np.sum(np.power((Y_predict), 2)) / (2*m)\n",
    "        print('The estimated cost before updation',cost1)\n",
    "        \n",
    "        for i in range(self.iterations):\n",
    "            Y_predict = self.predict(X) - Y\n",
    "            error = np.dot(X.T, Y_predict)\n",
    "            self.fit_intercept = self.fit_intercept - (self.learning_rate/m) * error\n",
    "            mean_error=(1/m)*np.sum((Y_predict.T), 2)\n",
    "            self.cost.append(mse)\n",
    "       \n",
    "       \n",
    "        print('Mean square error:',self.mse)\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "#print(trainset_X)#354*13\n",
    "#print(trainset_Y)#354*1\n",
    "\n",
    "lr=linear_regression()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KcqYa1U3iRQ1"
   },
   "source": [
    "## Problem 1.4 (5 points)\n",
    "\n",
    "1. How do you interpret that a variable causes a model's mean square error to increase? (2 points)\n",
    "  - Answer: \n",
    "  Individually if I evaluate the function for the input data set with individual parameters at a time, the output error would vary. For those parameters that have the most error value ,ie, dfifference between the actual and predicted values, gives the variable that causes the most mean square error. For all the cases where the error squared difference is most are the parameters causing the most noise in the tested system.\n",
    "  Checking individually y=beta*x+c for parametrs beta gives the error prediction and eventually the mse of that dataset.\n",
    "2. Why we would want to normalize our variables? (1 point)\n",
    "  - Answer:\n",
    "  When the range of features does not differ uniformly, as in ,if some range from 0 to 100 and the orher one from 0 to 1000, we need to scale down the larger parameters to bring the values closr to mean and thereby reduce the deviation. When we do not  normalize, the elements form an elliptical path around the minimum position. When we scale down the values , the plot turns into a concentric circles almost equally deviated from mean each side.The path to the minimum is easier to traverse in such case.This makes the convergence fater.\n",
    "3. A model fitted using the exact same split dataset with normalized values will generate the same coefficients as a model that was fitted using values that haven't been normalized. Clearly state whether that statement is true or false and explain your reasoning. (2 points)\n",
    "  - Answer:\n",
    " Statement is not completely true.In most cases the coefficients remain the same rather than varying too much. Not normalizing makes the data space a not so smooth function giving haphazard coefficients . Also training the set would take too much time to reach the global minimum if the dataset is not normalized. Also the proportion of weights of the train and test dataset remain the same resulting in similar value coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RJSot41BkMrB"
   },
   "source": [
    "# Problem 2: Binary Classification\n",
    "\n",
    "## Problem 2.1 (5 points)\n",
    "\n",
    "Consider the binary classification problem of mapping a given input to two classes. Let $\\mathcal{X}=\\mathbb{R}^d$ and $\\mathcal{Y}=\\{+1, -1\\}$ be the input space and output space, respectively. In simple words, it means that the input has $d$ features and all of them are real valued, whereas the output can only take values $-1$ or $+1$. This is one of the most common problems in machine learning and many sophisticated methods exist to solve it. In the question, we will solve it using the concepts we have already learned in class. Let us assume the two sets of points can be separated using a straight line i.e. the samples are linearly separable. So if $d=2$, one should be able to draw a line to distinguish between the two classes. All points lying on side of the line should belong to a particular class (say $1$) and the points lying on the other side should belong to another class (say $2$). To see what this would look like,  your first task is as follows:\n",
    "\n",
    "Write a function that will randomly generate a dataset for this problem. Your function should randomly choose a line $l$, which can be denoted as $ax + by + c = 0$. According to basic high school geometry, the line divides the plane into two sides. On one side, $ax+by+c>0$ while on the other $ax+by+c<0$. Use this fact to randomly generate $k_0$ points on the side of class 0 (i.e. $y=-1$) and $k_1$ points on the side of class 1 (i.e. $y=1$). Create a plot of this dataset where all the points corresponding to one class are blue and those of the other class are green, the line dividing both classes should be red. Axes should be labeled.\n",
    "\n",
    "**Note**: Do not confuse the $x$ and $y$ in the equation of line $ax + by + c = 0$ with $\\mathcal{X} $ and $\\mathcal{Y}$. Instead imagine these $x$ and $y$ as the 2-D coordinate system on which you have different points which should lie on 2 sides of the line $ax + by + c = 0$. For example, there is a point (2,3) in the 2-D system where $x = 2$ and $y = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g96jFpGyMIFu"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def generate_dataset(k0, k1):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    k0 : integer, number of samples for class 0\n",
    "    k1 : integer, number of samples for class 1\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : array, shape (m, d), dimension numpy array where m is the number of \n",
    "    samples and d is the number of features \n",
    "\n",
    "    Y : array, (m, 1), dimension vector where m is the number of samples\n",
    "    \"\"\"\n",
    "    a=random.uniform(-10,10)\n",
    "    print('a:',a)\n",
    "    num_points=10\n",
    "    b=random.uniform(-10,10)\n",
    "    print('b:',b)\n",
    "    c=random.uniform(-10,10)\n",
    "    print('c:',c)\n",
    "    \n",
    "    x=random.sample(range(-30,30), 10)\n",
    "    \n",
    "    y=[]\n",
    "    for i in range(len(x)):\n",
    "        \n",
    "        y.append(-((a*x[i])+c)/b)\n",
    "        \n",
    "    plt.plot(x,y,color = 'red')\n",
    "    \n",
    "    data,target=[],[]\n",
    "    \n",
    "    k0x, k0y = [],[]\n",
    "    while len(data) <= k0:\n",
    "        x, y = random.uniform(-20,20), random.uniform(-20,20)\n",
    "        if (a*x)+(b*y)+c < 0: \n",
    "            k0x.append(x) \n",
    "            k0y.append(y)\n",
    "            data.append([x,y])\n",
    "            target.append(-1)\n",
    "                \n",
    "    k1x, k1y = [],[]\n",
    "    while len(data) <= (k0+k1):\n",
    "        x, y = random.uniform(-20,20), random.uniform(-20,20)\n",
    "        if (a*x)+(b*y)+c > 0:\n",
    "            k1x.append(x)\n",
    "            k1y.append(y)\n",
    "            data.append([x,y])\n",
    "            target.append(1)\n",
    "            \n",
    "    plt.scatter(k0x,k0y,color ='b',label='-1')\n",
    "    plt.scatter(k1x,k1y,color ='g',label='1')\n",
    "    plt.xlabel('x-Axis')\n",
    "    plt.ylabel('y-Axis')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return np.array(data), np.array(target)\n",
    "\n",
    "X,Y = generate_dataset(30,15)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9mw15CFikXI1"
   },
   "source": [
    "## Problem 2.2 (35 points)\n",
    "\n",
    "If $\\mathcal{Y}$ is the variable you are trying to predict using a feature $\\mathcal{X}$ then in a typical Machine Learning problem, you are tasked with a target function $f$ which maps $\\mathcal{X}$ to $\\mathcal{Y}$ i.e. Find $f$ such that  $\\mathcal{Y}$  = $f(\\mathcal{X})$\n",
    "\n",
    "\n",
    "When you are given a dataset for which you do not have access the target function $f$, you have to learn it from the data. In this problem, we are going to learn the parameters of the line that separates the two classes for the dataset that we constructed in Problem 2.1. As we previously mentioned, that line can be represented as $ax + by + c = 0$.\n",
    "\n",
    "The goal here is to correctly find out the coefficients $a$, $b$, and $c$, represented below as $\\bf{w}$ which is a vector. The algorithm to find it is a simple iterative process: \n",
    "\n",
    "1. Randomly choose a $\\mathbf{w}$ to begin with.\n",
    "2. Keep on adjusting the value of $\\bf{w}$ as follows until all data samples are correctly classified:\n",
    "    1. Randomly choose a sample from the dataset without replacement and see if it is correctly classified. If yes,  move on to another sample.\n",
    "    2. If not,  then  update the weights as $\\mathbf{w}^{t+1} = \\mathbf{w}^t + y \\cdot \\mathbf{x}$\n",
    "    and go back to the previous step (of randomly chosing a sample)\n",
    "    \n",
    "        - $\\mathbf{w}^{t+1}$ is value of $\\mathbf{w}$ at iteration $t+1$\n",
    "        - $\\mathbf{w}^{t}$ is value of $\\mathbf{w}$ at iteration $t$\n",
    "        - $y$ is the class label for the sample under consideration\n",
    "        - $\\mathbf{x}$ is the data-point under consideration\n",
    "    \n",
    "    \n",
    "Write a function that implements this learning algorithm. The input to the function is going to be a dataset represented by the input variable $X$ and the target variable $y$. The output of the function should be the chosen $\\mathbf{w}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SPk7AZaLkXSh"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def generate_dataset(k0, k1):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    k0 : integer, number of samples for class 0\n",
    "    k1 : integer, number of samples for class 1\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : array, shape (m, d), dimension numpy array where m is the number of \n",
    "    samples and d is the number of features \n",
    "\n",
    "    Y : array, (m, 1), dimension vector where m is the number of samples\n",
    "    \"\"\"\n",
    "    a=random.uniform(-10,10)\n",
    "    print('a:',a)\n",
    "    num_points=10\n",
    "    b=random.uniform(-10,10)\n",
    "    print('b:',b)\n",
    "    c=random.uniform(-10,10)\n",
    "    print('c:',c)\n",
    "    \n",
    "    x=random.sample(range(-30,30), 10)\n",
    "    \n",
    "    y=[]\n",
    "    for i in range(len(x)):\n",
    "        \n",
    "        y.append(-((a*x[i])+c)/b)\n",
    "        \n",
    "    plt.plot(x,y,color = 'red')\n",
    "    \n",
    "    data,target=[],[]\n",
    "    \n",
    "    k0x, k0y = [],[]\n",
    "    while len(data) <= k0:\n",
    "        x, y = random.uniform(-20,20), random.uniform(-20,20)\n",
    "        if (a*x)+(b*y)+c < 0: \n",
    "            k0x.append(x) \n",
    "            k0y.append(y)\n",
    "            data.append([x,y])\n",
    "            target.append(-1)\n",
    "                \n",
    "    k1x, k1y = [],[]\n",
    "    while len(data) <= (k0+k1):\n",
    "        x, y = random.uniform(-20,20), random.uniform(-20,20)\n",
    "        if (a*x)+(b*y)+c > 0:\n",
    "            k1x.append(x)\n",
    "            k1y.append(y)\n",
    "            data.append([x,y])\n",
    "            target.append(1)\n",
    "            \n",
    "    plt.scatter(k0x,k0y,color ='b',label='-1')\n",
    "    plt.scatter(k1x,k1y,color ='g',label='1')\n",
    "    plt.xlabel('x-Axis')\n",
    "    plt.ylabel('y-Axis')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return np.array(data), np.array(target)\n",
    "\n",
    "X,Y = generate_dataset(30,15)\n",
    "#print(X.shape) #46*2   \n",
    "#M=len(X[0])+1#3\n",
    "#M\n",
    "\n",
    "def fit_line(X, y):\n",
    "    \"\"\"Predict using the binary classification model. Use the dataset generated \n",
    "    using generate_data() as input for this function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like, shape (n_samples, n_features)\n",
    "        Samples.\n",
    "    y : array_like, shape (n_labels, 1)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w : array, shape (1,n_features)\n",
    "        Returns the final weight vector w.  \n",
    "    \"\"\"\n",
    "    w=np.array([0.0] * (1+len(X[0])))\n",
    "    ones = np.ones((len(X),1))\n",
    "    X = np.hstack((ones, X))\n",
    "    \n",
    "    lowerbound_x, lowerbound_y, upperbound_x, upperbound_y = [], [], [], []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        if Y[i] == -1: \n",
    "            lowerbound_x.append(X[:,1][i]) \n",
    "            lowerbound_y.append(X[:,2][i])\n",
    "\n",
    "        if Y[i] == 1: \n",
    "            upperbound_x.append(X[:,1][i]) \n",
    "            upperbound_y.append(X[:,2][i])\n",
    "\n",
    "        if np.dot(X[i], w.T) != Y[i]:\n",
    "            m=Y[i] * X[i]\n",
    "            w = w + m\n",
    "            \n",
    "    X_plot, Y_plot = X[:,1], []\n",
    "    \n",
    "    Y_plot=[]\n",
    "    for i in range(len(X)):\n",
    "        Y_plot.append(-(w[0] + (w[1] * X[i][1]))/w[2])\n",
    "    \n",
    "    plt.scatter(lowerbound_x, lowerbound_y, color = 'blue',label='-1') \n",
    "    plt.scatter(upperbound_x, upperbound_y, color = 'green',label='1')\n",
    "    plt.xlabel('X-Axis'),plt.ylabel('Y-Axis')\n",
    "    plt.plot(X_plot, Y_plot, color = 'red')\n",
    "    print(\"The corresponding weights are %s\" %(w))\n",
    "    return w\n",
    "\n",
    "fit_line(X,Y)\n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHc3t4e9MOxu"
   },
   "source": [
    "### Problem 2.3 (10 points)\n",
    "- Give an intuition of why the above algorithm converges for linearly separable data? We do not expect you to give a mathematic proof, but it would be great if you can provide one. You will get full points even if you just provide an intuition of a few lines. Including figures or mathematical equations is encouraged but not required. (5 points)\n",
    "\n",
    "  - Answer: \n",
    "  The above algorithm best fit for the generated dataset by taking the estimated values of y calculated using the parameters and data matrix. Then it takes these values to calculate the lower and upper bounds for the line. Eventually based on these results it plots a separation line. This line gets updated to accurately fit the points by calculating the product of transpose(vector w) and X rows to get the bias line corrected to the accurate position. \n",
    "\n",
    "- What happens when the data is not linearly separable? What can be done to salvage the situation?\n",
    "\n",
    "  - Answer:\n",
    "  The function we used to predict best fit line only linearly classifies the data. But this is not useful to describe/classify the data in an accurate way. The data points at or near the junction get linearly classified into the other class if they fall above the best fit line. For describing the data accurately, we need a curvy separation for exactly dividing the classes separetly. We cannot determine non linear data with a straight line function.\n",
    "  When the data is non-linearly separated, there is a high chance that the points get merged into other classes. For this type of classification, we can no longer use the linear separation line.\n",
    "  For solving such type of equations, we need to use higher degree polynomial regression to classify the data clusters more accurately."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "A1-F19.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
